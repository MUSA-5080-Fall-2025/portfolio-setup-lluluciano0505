{
  "hash": "d9c728ebaa57561b4da2b78ed215382b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space-Time Prediction of Bike Share Demand: Philadelphia Indego\"\nauthor: \"Luciano\"\ndate: \"2025-11-21\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n    code_folding: show\n    code_download: true\n---\n# Setup\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\n```\n:::\n\n\n## Define Themes\nThese themes will be used for consistent styling of plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q1 2025 data\nlibrary(readr)\nindego <- read_csv(\"data/indego-trips-2024-q2.csv\")\n\n# Quick look at the data\nglimpse(indego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 368,091\nColumns: 15\n$ trip_id             <dbl> 867727465, 867732652, 867732304, 867732603, 867732…\n$ duration            <dbl> 8, 19, 9, 16, 7, 255, 56, 145, 37, 4, 40, 6, 3, 8,…\n$ start_time          <chr> \"4/1/2024 0:00\", \"4/1/2024 0:03\", \"4/1/2024 0:04\",…\n$ end_time            <chr> \"4/1/2024 0:08\", \"4/1/2024 0:22\", \"4/1/2024 0:13\",…\n$ start_station       <dbl> 3168, 3317, 3104, 3295, 3041, 3075, 3112, 3362, 30…\n$ start_lat           <dbl> 39.95134, 39.93633, 39.96664, 39.95028, 39.96849, …\n$ start_lon           <dbl> -75.17394, -75.19447, -75.19209, -75.16027, -75.13…\n$ end_station         <dbl> 3365, 3359, 3104, 3119, 3376, 3075, 3112, 3253, 30…\n$ end_lat             <dbl> 39.96173, 39.94888, 39.96664, 39.96674, 39.97183, …\n$ end_lon             <dbl> -75.18788, -75.16978, -75.19209, -75.20799, -75.14…\n$ bike_id             <chr> \"24987\", \"11555\", \"31244\", \"23243\", \"03696\", \"2535…\n$ plan_duration       <dbl> 30, 30, 365, 30, 365, 30, 1, 30, 30, 365, 1, 30, 3…\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"Round Trip\", \"One Way\", \"On…\n$ passholder_type     <chr> \"Indego30\", \"Indego30\", \"Indego365\", \"Indego30\", \"…\n$ bike_type           <chr> \"electric\", \"standard\", \"electric\", \"electric\", \"s…\n```\n\n\n:::\n:::\n\n---\n\n# 1. Introduction \nThis report analyzes hourly bike share demand in Philadelphia using Indego data from Q2 2024. The goal is to build and evaluate space-time predictive models, diagnose where and why errors occur, and propose new features that improve forecasting performance for operational planning.\n\n\n\n---\n\n## Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q2 2024:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q2 2024: 368091 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1711929600 to 1719791760 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 255 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    342299      25792 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Day Pass  Indego30 Indego365 \n    20926    231909    115256 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  216497   151594 \n```\n\n\n:::\n:::\n\n\n## Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2024-04-01 00:00:00 2024-04-01 00:00:00    14 Mon       0       0\n2 2024-04-01 00:03:00 2024-04-01 00:00:00    14 Mon       0       0\n3 2024-04-01 00:04:00 2024-04-01 00:00:00    14 Mon       0       0\n4 2024-04-01 00:04:00 2024-04-01 00:00:00    14 Mon       0       0\n5 2024-04-01 00:06:00 2024-04-01 00:00:00    14 Mon       0       0\n6 2024-04-01 00:07:00 2024-04-01 00:00:00    14 Mon       0       0\n```\n\n\n:::\n:::\n\n\n# 2. Exploratory Data Analysis  \n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q2 2024\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/trips_over_time-1.png){width=672}\n:::\n:::\n\nDaily trip volumes rise sharply from early April into mid-June, consistent with the seasonal transition into warmer weather. The series remains noisy—weekends, rain events, and holiday fluctuations introduce large day-to-day swings—but the overall trend is a steady climb toward peak summer demand. Compared to Q1 2025, the level is higher and the pattern is less stable, reflecting how warmer months attract more discretionary, non-commuting trips.\n\n## Hourly Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/hourly_patterns-1.png){width=672}\n:::\n:::\n\n\nHourly usage patterns retain the classic two-peak commute structure on weekdays, with strong surges around 8 AM and 5 PM. Weekends, by contrast, show a smoother, flatter trajectory centered on late morning and mid-afternoon leisure travel. The contrast suggests that Q2 demand mixes both utilitarian and recreational usage, producing more varied temporal dynamics than in winter.\n\n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 6,115 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 5,231 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,295 </td>\n   <td style=\"text-align:right;\"> 39.95028 </td>\n   <td style=\"text-align:right;\"> -75.16027 </td>\n   <td style=\"text-align:right;\"> 4,451 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 4,248 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 4,070 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 4,052 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 4,047 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 3,847 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 3,792 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,208 </td>\n   <td style=\"text-align:right;\"> 39.95048 </td>\n   <td style=\"text-align:right;\"> -75.19324 </td>\n   <td style=\"text-align:right;\"> 3,661 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,052 </td>\n   <td style=\"text-align:right;\"> 39.94732 </td>\n   <td style=\"text-align:right;\"> -75.15695 </td>\n   <td style=\"text-align:right;\"> 3,651 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,012 </td>\n   <td style=\"text-align:right;\"> 39.94218 </td>\n   <td style=\"text-align:right;\"> -75.17747 </td>\n   <td style=\"text-align:right;\"> 3,573 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,163 </td>\n   <td style=\"text-align:right;\"> 39.94974 </td>\n   <td style=\"text-align:right;\"> -75.18097 </td>\n   <td style=\"text-align:right;\"> 3,558 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,244 </td>\n   <td style=\"text-align:right;\"> 39.93865 </td>\n   <td style=\"text-align:right;\"> -75.16674 </td>\n   <td style=\"text-align:right;\"> 3,549 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 3,523 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,007 </td>\n   <td style=\"text-align:right;\"> 39.94517 </td>\n   <td style=\"text-align:right;\"> -75.15993 </td>\n   <td style=\"text-align:right;\"> 3,492 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 3,470 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 3,443 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 3,425 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 3,404 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n# 3. Get Philadelphia Spatial Context\n\n## Load Philadelphia Census Data\n\nWe'll get census tract data to add demographic context to our stations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/map_philly-1.png){width=672}\n:::\n:::\n\n\nThe spatial distribution of median household income shows a familiar pattern: higher-income tracts cluster in Center City, University City, and along the northwest corridor, while lower-income neighborhoods are concentrated in North and Southwest Philadelphia.\n\n## Join Census Data to Stations\n\nWe'll spatially join census characteristics to each bike station.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/join_census_to_stations-1.png){width=672}\n:::\n:::\n\n\nMost Indego stations successfully join to their surrounding census tract, but a subset in the downtown core and waterfront areas fall outside tract boundaries or sit within non-residential zones. These locations—shown as red X marks—typically correspond to transit hubs, tourist destinations, or large institutional complexes. Their absence from the demographic join highlights where neighborhood characteristics may not meaningfully describe station demand, and it motivates either separate modeling or additional spatial features for these atypical sites.\n\n## Dealing with missing data\n\nWe need to decide what to do with the non-residential bike share stations. For this example, we are going to remove them -- this is not necessarily the right way to do things always, but for the sake of simplicity, we are narrowing our scope to only stations in residential neighborhoods. We might opt to create a separate model for non-residential stations..\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n\n\n## Get Weather Data\n\nWeather significantly affects bike share demand! Let's get hourly weather for Philadelphia.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q2 2024: April 1 - June 30\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-04-01\",\n  date_end = \"2024-06-30\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature    Precipitation       Wind_Speed    \n Min.   :37.00   Min.   :0.00000   Min.   : 0.000  \n 1st Qu.:54.00   1st Qu.:0.00000   1st Qu.: 5.000  \n Median :66.00   Median :0.00000   Median : 7.000  \n Mean   :65.05   Mean   :0.00794   Mean   : 7.677  \n 3rd Qu.:74.00   3rd Qu.:0.00000   3rd Qu.:10.000  \n Max.   :98.00   Max.   :1.05000   Max.   :30.000  \n```\n\n\n:::\n:::\n\n\n## Visualize Weather Patterns\n\nWho is ready for a Philly winter?!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q2 2024\",\n    subtitle = \"Spring to Summer transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/visualize_weather-1.png){width=672}\n:::\n:::\n\n\nTemperatures rise steadily from early April into late June, marking the transition into Philadelphia’s warm season. Daily fluctuations remain sharp, but the overall shift from cool spring days to consistently warm summer weather is clear. This warming pattern aligns with the strong seasonal increase in ridership, reinforcing the well-known sensitivity of bike share demand to comfortable outdoor conditions.\n\n## Create Space-Time Panel\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 175936\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 235\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2184\n```\n\n\n:::\n:::\n\n\n## Create Complete Panel Structure\n\nNot every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 513,240 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 175,936 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 337,304 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 513,240 \n```\n\n\n:::\n:::\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %>% select(Trip_Count, Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count       Temperature    Precipitation   \n Min.   : 0.0000   Min.   :37.00   Min.   :0.0000  \n 1st Qu.: 0.0000   1st Qu.:54.00   1st Qu.:0.0000  \n Median : 0.0000   Median :66.00   Median :0.0000  \n Mean   : 0.6415   Mean   :65.05   Mean   :0.0079  \n 3rd Qu.: 1.0000   3rd Qu.:74.00   3rd Qu.:0.0000  \n Max.   :22.0000   Max.   :98.00   Max.   :1.0500  \n                   NA's   :5640    NA's   :5640    \n```\n\n\n:::\n:::\n\n\n## Create Temporal Lag Variables\n\nThe key innovation for space-time prediction: **past demand predicts future demand**.\n\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 632,150 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/lag_correlations-1.png){width=672}\n:::\n:::\n\n\nTrip counts at a single station show clear short-term persistence: hours with activity are typically followed by additional activity in the next hour, while long stretches of zero demand also cluster together. The 1-hour lag tracks the current series most closely, but the 24-hour lag preserves the broader daily rhythm. These patterns justify the use of temporal lag features in the predictive models, since recent demand provides meaningful information about expected short-run usage.\n\n## Temporal Train/Test Split\n\n**CRITICAL:** We must train on PAST data and test on FUTURE data!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# Q1 has weeks 14-26 (April-June)\n# Train on weeks 14-23 (April 1 - early June)\n# Test on weeks 23-26 (rest of June)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 23)\n\ntest <- study_panel_complete %>%\n  filter(week >= 23)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 447,528 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 176,552 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 19814 to 19876 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 19877 to 19904 \n```\n\n\n:::\n:::\n\n\n\n# 4. Build Predictive Models\n\nWe'll build 5 models with increasing complexity to see what improves predictions.\n\n## Model 1: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7197 -0.6759 -0.2053  0.1850 20.6050 \n\nCoefficients:\n                   Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.623380   0.014322 -43.527 < 0.0000000000000002 ***\nas.factor(hour)1  -0.072867   0.011938  -6.104 0.000000001037041260 ***\nas.factor(hour)2  -0.100186   0.011862  -8.446 < 0.0000000000000002 ***\nas.factor(hour)3  -0.101366   0.011827  -8.571 < 0.0000000000000002 ***\nas.factor(hour)4  -0.098172   0.011994  -8.185 0.000000000000000272 ***\nas.factor(hour)5   0.031520   0.011964   2.635             0.008424 ** \nas.factor(hour)6   0.251304   0.011708  21.464 < 0.0000000000000002 ***\nas.factor(hour)7   0.539861   0.011720  46.065 < 0.0000000000000002 ***\nas.factor(hour)8   0.854340   0.011759  72.652 < 0.0000000000000002 ***\nas.factor(hour)9   0.645589   0.011734  55.020 < 0.0000000000000002 ***\nas.factor(hour)10  0.548414   0.011459  47.857 < 0.0000000000000002 ***\nas.factor(hour)11  0.579472   0.011564  50.111 < 0.0000000000000002 ***\nas.factor(hour)12  0.626987   0.011257  55.699 < 0.0000000000000002 ***\nas.factor(hour)13  0.623662   0.011744  53.104 < 0.0000000000000002 ***\nas.factor(hour)14  0.656044   0.011488  57.107 < 0.0000000000000002 ***\nas.factor(hour)15  0.760629   0.011788  64.526 < 0.0000000000000002 ***\nas.factor(hour)16  0.862528   0.011435  75.426 < 0.0000000000000002 ***\nas.factor(hour)17  1.157733   0.011763  98.420 < 0.0000000000000002 ***\nas.factor(hour)18  0.930235   0.011977  77.668 < 0.0000000000000002 ***\nas.factor(hour)19  0.681722   0.011792  57.813 < 0.0000000000000002 ***\nas.factor(hour)20  0.432304   0.011798  36.642 < 0.0000000000000002 ***\nas.factor(hour)21  0.269929   0.011696  23.078 < 0.0000000000000002 ***\nas.factor(hour)22  0.191038   0.011726  16.291 < 0.0000000000000002 ***\nas.factor(hour)23  0.069112   0.011871   5.822 0.000000005812931084 ***\ndotw_simple2       0.061945   0.006312   9.814 < 0.0000000000000002 ***\ndotw_simple3       0.021134   0.006089   3.471             0.000519 ***\ndotw_simple4       0.103407   0.006324  16.352 < 0.0000000000000002 ***\ndotw_simple5       0.057970   0.006276   9.237 < 0.0000000000000002 ***\ndotw_simple6       0.007771   0.006527   1.191             0.233767    \ndotw_simple7      -0.034128   0.006504  -5.247 0.000000154440328777 ***\nTemperature        0.012729   0.000171  74.419 < 0.0000000000000002 ***\nPrecipitation     -2.054152   0.062168 -33.042 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.117 on 447496 degrees of freedom\nMultiple R-squared:  0.1143,\tAdjusted R-squared:  0.1142 \nF-statistic:  1863 on 31 and 447496 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n\n## Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1097 -0.4314 -0.1254  0.1217 17.3211 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.2516703  0.0123654 -20.353 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0315610  0.0102751  -3.072             0.002129 ** \nas.factor(hour)2  -0.0255020  0.0102137  -2.497             0.012531 *  \nas.factor(hour)3  -0.0193838  0.0101872  -1.903             0.057071 .  \nas.factor(hour)4  -0.0118938  0.0103351  -1.151             0.249811    \nas.factor(hour)5   0.0879106  0.0103161   8.522 < 0.0000000000000002 ***\nas.factor(hour)6   0.2413446  0.0101023  23.890 < 0.0000000000000002 ***\nas.factor(hour)7   0.3912950  0.0101257  38.644 < 0.0000000000000002 ***\nas.factor(hour)8   0.5751810  0.0101693  56.561 < 0.0000000000000002 ***\nas.factor(hour)9   0.2586771  0.0101578  25.466 < 0.0000000000000002 ***\nas.factor(hour)10  0.2239928  0.0099018  22.621 < 0.0000000000000002 ***\nas.factor(hour)11  0.2750789  0.0099905  27.534 < 0.0000000000000002 ***\nas.factor(hour)12  0.3434636  0.0097188  35.340 < 0.0000000000000002 ***\nas.factor(hour)13  0.3325602  0.0101359  32.810 < 0.0000000000000002 ***\nas.factor(hour)14  0.3735677  0.0099128  37.686 < 0.0000000000000002 ***\nas.factor(hour)15  0.4382105  0.0101782  43.054 < 0.0000000000000002 ***\nas.factor(hour)16  0.5186079  0.0098810  52.485 < 0.0000000000000002 ***\nas.factor(hour)17  0.7372223  0.0101810  72.412 < 0.0000000000000002 ***\nas.factor(hour)18  0.3982131  0.0103986  38.295 < 0.0000000000000002 ***\nas.factor(hour)19  0.2607371  0.0102099  25.538 < 0.0000000000000002 ***\nas.factor(hour)20  0.1041311  0.0102076  10.201 < 0.0000000000000002 ***\nas.factor(hour)21  0.0773039  0.0100888   7.662  0.00000000000001829 ***\nas.factor(hour)22  0.0804148  0.0100993   7.962  0.00000000000000169 ***\nas.factor(hour)23  0.0141841  0.0102172   1.388             0.165062    \ndotw_simple2       0.0186587  0.0054346   3.433             0.000596 ***\ndotw_simple3      -0.0004020  0.0052441  -0.077             0.938896    \ndotw_simple4       0.0308340  0.0054469   5.661  0.00000001507660401 ***\ndotw_simple5       0.0029701  0.0054080   0.549             0.582861    \ndotw_simple6      -0.0249257  0.0056246  -4.432  0.00000935943656583 ***\ndotw_simple7      -0.0338659  0.0055995  -6.048  0.00000000146779797 ***\nTemperature        0.0037869  0.0001493  25.370 < 0.0000000000000002 ***\nPrecipitation     -0.9449920  0.0535786 -17.637 < 0.0000000000000002 ***\nlag1Hour           0.4030038  0.0013966 288.563 < 0.0000000000000002 ***\nlag3Hours          0.1234155  0.0013804  89.408 < 0.0000000000000002 ***\nlag1day            0.1253160  0.0012810  97.830 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.961 on 447493 degrees of freedom\nMultiple R-squared:  0.344,\tAdjusted R-squared:  0.344 \nF-statistic:  6903 on 34 and 447493 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0962 -0.6781 -0.2691  0.4234 17.2038 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.3857918409  0.0371091868  10.396\nas.factor(hour)1         -0.0021036158  0.0412694199  -0.051\nas.factor(hour)2         -0.0088620570  0.0469025109  -0.189\nas.factor(hour)3         -0.0851082042  0.0529075781  -1.609\nas.factor(hour)4         -0.1044632428  0.0557223076  -1.875\nas.factor(hour)5          0.0516286807  0.0371091993   1.391\nas.factor(hour)6          0.2505851189  0.0312856619   8.010\nas.factor(hour)7          0.4005664111  0.0294980575  13.579\nas.factor(hour)8          0.6170072423  0.0287282776  21.477\nas.factor(hour)9          0.1382271180  0.0288552539   4.790\nas.factor(hour)10         0.1084529302  0.0287107985   3.777\nas.factor(hour)11         0.1546773002  0.0286616605   5.397\nas.factor(hour)12         0.2287878417  0.0281439459   8.129\nas.factor(hour)13         0.2593480167  0.0286602368   9.049\nas.factor(hour)14         0.2789001366  0.0282806593   9.862\nas.factor(hour)15         0.3576516996  0.0284181466  12.585\nas.factor(hour)16         0.4807434184  0.0279992423  17.170\nas.factor(hour)17         0.8226919934  0.0281755098  29.199\nas.factor(hour)18         0.3568673063  0.0285563485  12.497\nas.factor(hour)19         0.1776173104  0.0286237376   6.205\nas.factor(hour)20         0.0116961313  0.0293501783   0.399\nas.factor(hour)21         0.0203613434  0.0299654314   0.679\nas.factor(hour)22         0.0304705944  0.0307099983   0.992\nas.factor(hour)23        -0.0160285584  0.0326523113  -0.491\ndotw_simple2              0.0547342235  0.0119297080   4.588\ndotw_simple3              0.0108137555  0.0116378166   0.929\ndotw_simple4              0.0381015979  0.0117220832   3.250\ndotw_simple5              0.0054589733  0.0118327085   0.461\ndotw_simple6              0.0459381855  0.0125566250   3.658\ndotw_simple7              0.0389827321  0.0126414408   3.084\nTemperature               0.0075088185  0.0003400333  22.083\nPrecipitation            -2.1950253458  0.1294673174 -16.954\nlag1Hour                  0.2941552792  0.0022571175 130.323\nlag3Hours                 0.0851609125  0.0023233110  36.655\nlag1day                   0.0958710202  0.0022118269  43.345\nMed_Inc.x                 0.0000001627  0.0000001133   1.435\nPercent_Taking_Transit.y -0.0032455105  0.0004140532  -7.838\nPercent_White.y           0.0032869902  0.0002079131  15.809\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                     0.959347    \nas.factor(hour)2                     0.850135    \nas.factor(hour)3                     0.107702    \nas.factor(hour)4                     0.060834 .  \nas.factor(hour)5                     0.164148    \nas.factor(hour)6         0.000000000000001159 ***\nas.factor(hour)7         < 0.0000000000000002 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9         0.000001666444693928 ***\nas.factor(hour)10                    0.000159 ***\nas.factor(hour)11        0.000000067997969458 ***\nas.factor(hour)12        0.000000000000000435 ***\nas.factor(hour)13        < 0.0000000000000002 ***\nas.factor(hour)14        < 0.0000000000000002 ***\nas.factor(hour)15        < 0.0000000000000002 ***\nas.factor(hour)16        < 0.0000000000000002 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18        < 0.0000000000000002 ***\nas.factor(hour)19        0.000000000547584341 ***\nas.factor(hour)20                    0.690260    \nas.factor(hour)21                    0.496826    \nas.factor(hour)22                    0.321099    \nas.factor(hour)23                    0.623508    \ndotw_simple2             0.000004477560354601 ***\ndotw_simple3                         0.352792    \ndotw_simple4                         0.001153 ** \ndotw_simple5                         0.644551    \ndotw_simple6                         0.000254 ***\ndotw_simple7                         0.002045 ** \nTemperature              < 0.0000000000000002 ***\nPrecipitation            < 0.0000000000000002 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                            0.151217    \nPercent_Taking_Transit.y 0.000000000000004594 ***\nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.221 on 145195 degrees of freedom\n  (302295 observations deleted due to missingness)\nMultiple R-squared:  0.2319,\tAdjusted R-squared:  0.2317 \nF-statistic:  1185 on 37 and 145195 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.259619 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.2582656 \n```\n\n\n:::\n:::\n\n\n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.2635307 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.2621691 \n```\n\n\n:::\n:::\n\n\n## Model Performance\nModel 1 — Time + Weather\nExplains only about 11% of variation. Captures basic hourly/weekly patterns and weather, but predictions are weak.\n\nModel 2 — + Temporal Lags\nBig improvement: R² jumps to 0.34. The 1-hour lag is the strongest predictor. Demand is highly path-dependent.\n\nModel 3 — + Demographics\nNo real improvement. Demographic variables add little to short-run hourly prediction.\n\nModel 4 — + Station Fixed Effects\nModerate gain (R² ≈ 0.26). Captures stable differences between stations, but less important than lag effects.\n\nModel 5 — Full Model\nSmall additional improvement (R² ≈ 0.264). Interactions help only slightly; most predictability still comes from recent demand.\n\nIn Q2 2024 specifically, the one-hour lag (`lag1Hour`) and 24-hour lag (`lag1day`) are by far the strongest predictors: hours with high recent demand almost always see high current demand. Temperature has a clear positive effect, while precipitation slightly suppresses ridership. By contrast, median income, racial composition, and transit share have tiny coefficients and do not materially change out-of-sample MAE, confirming that short-run dynamics matter much more than static neighborhood traits in this quarter.\n\n\n## Comparison to Q1 2025\n\nCompared to the original Q1 2025 analysis, the best Q2 2024 model achieves a similar order of magnitude of error, with an MAE of about 0.71 trips per station-hour. The key difference is that Q2 demand is higher and more volatile: warm-weather leisure and discretionary trips introduce more noise than winter commute patterns, so the model must cope with sharper peaks and more irregular usage. As a result, the overall fit is comparable, but prediction is more challenging exactly in the busiest summer periods.\n\n\n\n# 5.Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.87 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.97 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/compare_models-1.png){width=672}\n:::\n:::\n\n\nModel 2 — the version with temporal lags — clearly performs best, with an MAE of about 0.71 trips, much lower than all other models. Model 1 performs reasonably (0.87), but the demographic model (Model 3) and fixed-effect models (Models 4–5) do not improve accuracy.\n\n\n## Compare Q2 2024 vs. Q1 2025\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmae_q2 <- mae_results \n\nmae_q1 <- tibble(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE_Q1_2025 = c(0.60, 0.50, 0.74, 0.73, 0.73)\n)\n\n\nmae_compare <- mae_q1 %>%\n  left_join(mae_q2, by = \"Model\") %>%\n  rename(MAE_Q2_2024 = MAE)\n\nkable(\n  mae_compare,\n  digits = 2,\n  caption = \"MAE comparison: Q1 2025 vs Q2 2024\",\n  col.names = c(\"Model\", \"MAE (Q1 2025)\", \"MAE (Q2 2024)\")\n) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>MAE comparison: Q1 2025 vs Q2 2024</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (Q1 2025) </th>\n   <th style=\"text-align:right;\"> MAE (Q2 2024) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n   <td style=\"text-align:right;\"> 0.87 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n   <td style=\"text-align:right;\"> 0.97 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nAcross all five models, Q2 2024 has consistently **lower MAE** than Q1 2025.  \nFor the best-performing model (Model 2, with temporal lags), MAE drops from **0.71** trips per station-hour in Q1 to **0.50** in Q2. The simple time–weather baseline also improves from **0.87** to **0.60**, while the more complex models (3–5) remain worse than the lag model in both quarters.\n\n\n## Space-Time Error Analysis: Observed vs. Predicted\n\nLet's use our best model (Model 2) for error analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 3 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/obs_vs_pred-1.png){width=672}\n:::\n:::\n\n\n\n## Spatial Error Patterns\n\nAre prediction errors clustered in certain parts of Philadelphia?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon, y = start_lat, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/spatial_errors-1.png){width=672}\n:::\n:::\n\n\nPrediction errors are highest in Center City and University City, the two areas with the strongest and most volatile bike activity. These stations experience sharp swings in demand—commuters, tourists, and university traffic—making them harder to model with simple hourly features. In contrast, outlying residential neighborhoods show lower errors because their usage patterns are more stable and predictable. The close alignment between “high average demand” and “high error” suggests that variability, rather than low volume, drives most of the model’s difficulty.\n\n## Temporal Error Patterns\n\nWhen are we most wrong?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/temporal_errors-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweekly_errors <- test %>%\n  group_by(week, weekend) %>%\n  summarise(\n    MAE        = mean(abs_error, na.rm = TRUE),\n    mean_error = mean(error,     na.rm = TRUE),\n    .groups    = \"drop\"\n  ) %>%\n  mutate(\n    day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"),\n    week = factor(week)\n  )\n\np_week_mae <- ggplot(weekly_errors,\n                     aes(x = week, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\",\n                               \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Weekly Pattern in Prediction Errors\",\n    subtitle = \"Mean Absolute Error by week and day type (test set)\",\n    x = \"Week of year\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme\n\np_week_bias <- ggplot(weekly_errors,\n                      aes(x = week, y = mean_error, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\",\n                               \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Weekly Under- / Over-Prediction\",\n    subtitle = \"Mean signed error by week and day type (test set)\",\n    x = \"Week of year\",\n    y = \"Mean Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme\n\ngridExtra::grid.arrange(p_week_mae, p_week_bias, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nAcross the four test weeks (23–26), prediction errors are fairly stable: weekday and weekend MAE stay in the 0.7–0.8 trip range, with only a slight peak in week 24. Bias is small in most weeks, but weeks 23–24 show mild over-prediction, especially on weekdays. In contrast, week 25 exhibits noticeable under-prediction on weekends, suggesting an unusually strong spike in weekend demand that the model did not anticipate. By week 26, signed errors return close to zero, indicating that systematic under/over-prediction does not persist over time.\n\n## Errors and Demographics\n\nAre prediction errors related to neighborhood characteristics?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](hw_5_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\nStation–level errors are only weakly related to neighborhood demographics. MAE tends to be slightly higher in tracts with higher median income and a larger share of white residents, while it declines in tracts where a higher share of commuters take transit. In other words, the model struggles a bit more in richer, whiter areas and performs better in transit-oriented neighborhoods.\n\nThese differences appear modest and are driven mainly by higher and more volatile demand at busy, centrally located stations, rather than systematic under-performance in disadvantaged communities, though any deployment should still monitor errors by neighborhood over time.\n\n\n# 6. Feature Engineering & Model Improvement  \n\n## Create New Features\n\nThe choice of new features is guided directly by the error analysis in Section 5. Spatially and temporally, the model struggled most during warm, high-demand peaks in Center City and University City – especially on PM rush hours and busy weekends – and we also saw clear weekly repetition in the demand patterns. To target these misspecifications, I focus on features that (i) sharpen the model’s sensitivity to “perfect” biking weather and (ii) capture medium-run weekly structure beyond the short-run lags already in Model 2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perfect weather indicator: 60–75F & no rain\ntrain <- train %>%\n  mutate(\n    perfect_weather = ifelse(Temperature >= 60 &\n                               Temperature <= 75 &\n                               Precipitation == 0, 1, 0)\n  )\n\ntest <- test %>%\n  mutate(\n    perfect_weather = ifelse(Temperature >= 60 &\n                               Temperature <= 75 &\n                               Precipitation == 0, 1, 0)\n  )\n\n# Same hour last week (t-168 lag)\ntrain <- train %>%\n  arrange(start_station, interval60) %>%\n  group_by(start_station) %>%\n  mutate(lag168 = lag(Trip_Count, 24 * 7)) %>%\n  ungroup()\n\ntest <- test %>%\n  arrange(start_station, interval60) %>%\n  group_by(start_station) %>%\n  mutate(lag168 = lag(Trip_Count, 24 * 7)) %>%\n  ungroup()\n\n# Rolling 7-day mean demand\nlibrary(zoo)\n\ntrain <- train %>%\n  arrange(start_station, interval60) %>%\n  group_by(start_station) %>%\n  mutate(roll7 = rollmean(Trip_Count, k = 24 * 7, fill = NA, align = \"right\")) %>%\n  ungroup()\n\ntest <- test %>%\n  arrange(start_station, interval60) %>%\n  group_by(start_station) %>%\n  mutate(roll7 = rollmean(Trip_Count, k = 24 * 7, fill = NA, align = \"right\")) %>%\n  ungroup()\n```\n:::\n\n\n\n## Improved Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_improved <- train %>%\n  drop_na(lag1Hour, lag3Hours, lag1day,\n          lag168, roll7, perfect_weather)\n\ntest_improved <- test %>%\n  drop_na(lag1Hour, lag3Hours, lag1day,\n          lag168, roll7, perfect_weather)\n\n# improved OLS model\nmodel_improved <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    perfect_weather + lag168 + roll7,\n  data = train_improved\n)\n\nsummary(model_improved)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + perfect_weather + \n    lag168 + roll7, data = train_improved)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7670 -0.4742 -0.1195  0.2492 17.5809 \n\nCoefficients:\n                   Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.347813   0.014132 -24.612 < 0.0000000000000002 ***\nas.factor(hour)1  -0.045320   0.010779  -4.204       0.000026189516 ***\nas.factor(hour)2  -0.052158   0.010826  -4.818       0.000001450054 ***\nas.factor(hour)3  -0.060508   0.010759  -5.624       0.000000018662 ***\nas.factor(hour)4  -0.049480   0.010929  -4.527       0.000005976126 ***\nas.factor(hour)5   0.067097   0.010881   6.166       0.000000000699 ***\nas.factor(hour)6   0.240965   0.010712  22.496 < 0.0000000000000002 ***\nas.factor(hour)7   0.412024   0.010738  38.371 < 0.0000000000000002 ***\nas.factor(hour)8   0.632008   0.010752  58.779 < 0.0000000000000002 ***\nas.factor(hour)9   0.339184   0.010779  31.466 < 0.0000000000000002 ***\nas.factor(hour)10  0.305729   0.010491  29.142 < 0.0000000000000002 ***\nas.factor(hour)11  0.366893   0.010621  34.545 < 0.0000000000000002 ***\nas.factor(hour)12  0.445347   0.010291  43.274 < 0.0000000000000002 ***\nas.factor(hour)13  0.430450   0.010679  40.306 < 0.0000000000000002 ***\nas.factor(hour)14  0.491254   0.010499  46.789 < 0.0000000000000002 ***\nas.factor(hour)15  0.566113   0.010813  52.354 < 0.0000000000000002 ***\nas.factor(hour)16  0.684121   0.010559  64.788 < 0.0000000000000002 ***\nas.factor(hour)17  0.923141   0.010919  84.543 < 0.0000000000000002 ***\nas.factor(hour)18  0.563153   0.011074  50.851 < 0.0000000000000002 ***\nas.factor(hour)19  0.407262   0.010920  37.296 < 0.0000000000000002 ***\nas.factor(hour)20  0.210928   0.010895  19.360 < 0.0000000000000002 ***\nas.factor(hour)21  0.153190   0.010764  14.231 < 0.0000000000000002 ***\nas.factor(hour)22  0.134214   0.010824  12.400 < 0.0000000000000002 ***\nas.factor(hour)23  0.043454   0.010766   4.036       0.000054350305 ***\ndotw_simple2       0.080028   0.005953  13.443 < 0.0000000000000002 ***\ndotw_simple3       0.048226   0.005684   8.484 < 0.0000000000000002 ***\ndotw_simple4       0.030337   0.005695   5.327       0.000000099800 ***\ndotw_simple5      -0.007292   0.005582  -1.306                0.191    \ndotw_simple6      -0.050845   0.005802  -8.763 < 0.0000000000000002 ***\ndotw_simple7      -0.058297   0.005756 -10.128 < 0.0000000000000002 ***\nTemperature        0.001028   0.000184   5.589       0.000000022844 ***\nPrecipitation     -2.853109   0.120123 -23.752 < 0.0000000000000002 ***\nlag1Hour           0.333305   0.001495 223.008 < 0.0000000000000002 ***\nlag3Hours          0.060813   0.001489  40.833 < 0.0000000000000002 ***\nlag1day            0.061350   0.001403  43.718 < 0.0000000000000002 ***\nperfect_weather    0.015343   0.003476   4.413       0.000010185951 ***\nlag168            -0.007327   0.001428  -5.133       0.000000285408 ***\nroll7              0.534007   0.004073 131.092 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9703 on 408514 degrees of freedom\nMultiple R-squared:  0.3657,\tAdjusted R-squared:  0.3657 \nF-statistic:  6367 on 37 and 408514 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n## Compare Improved Model to Baseline Model 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# baseline Model 2 predictions\npred_base <- predict(model2, newdata = test_improved)\n# improved model predictions\npred_improved <- predict(model_improved, newdata = test_improved)\n\n# just in case, drop any rows where predictions are NA\nvalid_idx <- !is.na(pred_base) & !is.na(pred_improved) & !is.na(test_improved$Trip_Count)\n\nmae_base <- mean(abs(test_improved$Trip_Count[valid_idx] - pred_base[valid_idx]))\nmae_improved <- mean(abs(test_improved$Trip_Count[valid_idx] - pred_improved[valid_idx]))\n\ncat(\"Model 2 MAE (baseline): \", round(mae_base, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 2 MAE (baseline):  0.721 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Improved Model MAE:     \", round(mae_improved, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImproved Model MAE:      0.729 \n```\n\n\n:::\n:::\n\nThe improved model is built by augmenting Model 2 (time, weather, and short-run lags) with the three new features.\nOn the held-out test set, however, performance does not improve:\nthe baseline Model 2 achieves an MAE of 0.72 trips, while the enhanced model’s MAE rises slightly to 0.73 trips.\n\nBreaking the errors down by station and time of day confirms that there is no clear subset where the new features deliver large gains: MAE by time period and by station changes only marginally, and the ranking of “easy” vs. “hard” stations is essentially unchanged. In other words, the extra features slightly reshuffle individual residuals but do not systematically fix the large errors identified in the earlier analysis..\n\n\n## Poisson Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pois <- glm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    perfect_weather + lag168 + roll7,\n  data = train_improved,\n  family = poisson(link = \"log\")\n)\n\npred_pois <- predict(model_pois, newdata = test_improved, type = \"response\")\nvalid_pois <- !is.na(pred_pois) & !is.na(test_improved$Trip_Count)\n\nmae_pois <- mean(abs(test_improved$Trip_Count[valid_pois] - pred_pois[valid_pois]))\n\ncat(\"Poisson MAE:            \", round(mae_pois, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoisson MAE:             0.729 \n```\n\n\n:::\n:::\n\n\nI also estimated a Poisson regression using the same feature set as the improved linear model. The Poisson model reaches an MAE of about 0.73 trips, essentially identical to the enhanced OLS model and still slightly worse than the simpler Model 2. This indicates that, although count data models are theoretically appealing, the Poisson specification does not offer practical gains here—likely because hourly Indego counts are over-dispersed and include many zeros.\n\n# 7. Brief Report Summary\n\n## Quarter and Motivation\nThis project uses **Q2 2024** Indego data (April–June). This quarter captures the transition into Philadelphia’s warm season, when bike share demand is higher, more volatile, and more influenced by discretionary and leisure trips than in winter. It therefore provides a stronger test of short-run demand prediction.\n\n## Model Comparison\nI estimate five nested OLS models. A simple time + weather model (Model 1) explains only about 11% of variation and yields an MAE of **0.87** trips. Adding temporal lag terms (Model 2) is crucial: R² rises to about **0.34** and MAE falls to **0.71**, making it the best-performing baseline. Demographics, station fixed effects, and rush-hour interactions (Models 3–5) increase model complexity but do not improve out-of-sample MAE. Recent demand at each station is far more predictive than static neighborhood characteristics.\n\n## Error Analysis\nSpatially, errors are highest at busy stations in Center City and University City, where commuter, tourist, and university traffic create sharp, irregular peaks. Peripheral residential stations are more predictable. Temporally, weekday PM peaks show the largest errors, while overnight hours are very stable. Demographically, errors are slightly higher in higher-income and whiter tracts, mostly because those areas host the busiest and most volatile stations.\n\n## New Features and Poisson Model\nTo improve Model 2, I add three features: a perfect-weather indicator, a same-hour-last-week lag, and a rolling 7-day average of demand. However, the enhanced model’s MAE rises slightly to **0.73**, and a Poisson count model with the same features also achieves about **0.73**. These results suggest that the short-run lag structure already captures most usable signal, and the additional temporal trend features mainly add noise in this three-month sample.\n\n## Critical Reflection\nAn MAE of roughly **0.7 trips per station-hour** is operationally useful for high-level planning and rebalancing strategy, but not sufficient for fully automated real-time control, especially during volatile PM peaks in the downtown core. Errors do not appear to systematically disadvantage low-income or high-transit neighborhoods, but any deployment should monitor error patterns by area and enforce minimum service standards in equity-priority communities. The current model ignores events, disruptions, and spatial spillovers; with more time and data, I would incorporate event calendars, network structure, and more flexible model families to better capture the complex dynamics of Philadelphia’s bike share system.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}